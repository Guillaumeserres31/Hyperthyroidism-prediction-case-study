# -*- coding: utf-8 -*-
"""EXAMEN AIBA PT 2024 Guillaume Serres .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14t4eUcjNQjlfROlGDOx1NGCjYC8fQN8Q
"""



"""# Data cleaning and preparation"""

###### Pre-treatment
# Import Packages
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from imblearn.over_sampling import SMOTE
# Loading the drive
from google.colab import drive
drive.mount('/content/drive')

#Importing the data file
file_path = '/content/drive/MyDrive/sampled_data21.csv'
data = pd.read_csv(file_path, sep=';')

#Check if the data have been imported correctly
print(data.head())

print(data.describe())

print(data.info())

# Check of missing values
data.isna().sum()

# There is no missing values

# Count of the number of hyperthyroidism case in the dataset
data["hyperthyroidism"].value_counts()

# There is a low proportion of sick invididuals in the dataset

print(data.dtypes)

"""# Data modelling and evaluations"""

# Separation of the output variable from the features
X = data.drop('hyperthyroidism', axis=1)
y = data['hyperthyroidism']

# Divide data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Random forest model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)
# Predict on the test set
y_pred = model.predict(X_test)
# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

print(f'Accuracy: {accuracy}')
print(f'Confusion Matrix:\n{conf_matrix}')
print(f'Classification Report:\n{class_report}')

# The low proportion of sick individuals limits the predictive capacity of the model, which predicts a large number of false negatives.
# We will therefore resample the dataset to obtain an equivalent number of healthy and sick individuals.

# resampling of the data using SMOTE
oversampler = SMOTE(random_state=42)
X_resampled, y_resampled = oversampler.fit_resample(X, y)

print(pd.Series(y_resampled).value_counts())

# By resampling the dataset using SMOTE, which generates examples based on existing ones, we obtain an equal number of sick and healthy people.
# We will now test different algorithms to determine which one best predicts that someone is positive for hyperthyroidism on the basis of the characteristics observed.

# Divide the data into training and test sets with the data resampled
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# Random forest Initialisation and training
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Predict on the test set
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

print(f'Accuracy: {accuracy}')
print(f'Confusion Matrix:\n{conf_matrix}')
print(f'Classification Report:\n{class_report}')

# Logistic Regression

# Initialise and train the model
model = LogisticRegression()
model.fit(X_train, y_train)

# Predict on the test set
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

print(f'Accuracy: {accuracy}')
print(f'Confusion Matrix:\n{conf_matrix}')
print(f'Classification Report:\n{class_report}')

# Classification K-nearest neighbors
from sklearn.neighbors import KNeighborsClassifier
# Initialise and train the model
model = KNeighborsClassifier(n_neighbors=2)
model.fit(X_train, y_train)

# Predict on the test set
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

print(f'Accuracy: {accuracy}')
print(f'Confusion Matrix:\n{conf_matrix}')
print(f'Classification Report:\n{class_report}')

# Gradient Boosting
from sklearn.ensemble import GradientBoostingClassifier

# Initialise and train the model
model = GradientBoostingClassifier(random_state=42)
model.fit(X_train, y_train)

# Predict on the test set
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

print(f'Accuracy: {accuracy}')
print(f'Confusion Matrix:\n{conf_matrix}')
print(f'Classification Report:\n{class_report}')

"""We observed that the Gradient Boosting and Random Forest models performed well and similarly, but it was the Gradient Boosting model that performed best, with a correct prediction rate of almost 93%. It also has the highest f1-score when combining recall and accuracy for both sick and healthy individuals.

**I therefore recommend that the Pitié Salpêtrière Hospital use this Gradient Boosting model** to be able to predict that someone will be positive for hyperthyroidism based on observed attributes.
"""